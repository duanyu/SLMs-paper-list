# paper list

## Knowledge Injection
SLMs are very suitable for speific domain with fine-tuning, and domain knowledge adaptation is crucial.

These papers discuss the methods of knowledge injection:
+ [Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs](https://arxiv.org/pdf/2312.05934)
+ [INJECTING NEW KNOWLEDGE INTO LARGE LANGUAGE MODELS VIA SUPERVISED FINE-TUNING](https://arxiv.org/pdf/2404.00213)
+ [Adapting Large Language Models via Reading Comprehension](https://arxiv.org/pdf/2309.09530)

## Fine-tuning SLMs for specific tasks/domain
+ [RAD-PHI2: INSTRUCTION TUNING PHI-2 FOR RADIOLOGY](https://arxiv.org/pdf/2403.09725)

## Architecture
Different architecture or SLMs:
+ [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/pdf/2402.14905)

## Agent
+ [Octopus: On-device language model for function calling of software APIs](https://arxiv.org/pdf/2404.01549)
+ [Octopus v2: On-device language model for super agent](https://arxiv.org/pdf/2404.01744)
+ [Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent](https://arxiv.org/pdf/2404.11459)
+ [Octopus v4: Graph of language models](https://arxiv.org/pdf/2404.19296)
